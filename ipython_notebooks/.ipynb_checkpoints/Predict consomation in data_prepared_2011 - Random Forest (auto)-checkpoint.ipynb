{
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting consomation in ECOMETERING_LIGHT.data_prepared_2011"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook automatically generated from your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generated on 2015-01-27 10:11:44.287767"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "prediction\nThis notebook will reproduce all the steps for a regression on  ECOMETERING_LIGHT.data_prepared_2011.\nThe main objective is to predict the variable consomation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s start with importing the required libs :"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport dataiku.core.pandasutils as pdu\nfrom dataiku.doctor.preprocessing import PCA\nfrom collections import defaultdict, Counter"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And tune pandas display options:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.set_option(\u0027display.width\u0027, 3000)\npd.set_option(\u0027display.max_rows\u0027, 200)\npd.set_option(\u0027display.max_columns\u0027, 200)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing base data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step is to get our machine learning dataset:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "input_columns \u003d [u\u0027temperature\u0027, u\u0027hour\u0027, u\u0027NAF_code\u0027, u\u0027Site\u0027, u\u0027month\u0027, u\u0027week_day\u0027, u\u0027consomation\u0027]\n%time ml_dataset \u003d dataiku.Dataset(\u0027ECOMETERING_LIGHT.data_prepared_2011\u0027).get_dataframe(columns\u003dinput_columns)\nprint \u0027Base data has %i rows and %i columns\u0027 % (ml_dataset.shape[0], ml_dataset.shape[1])\n# Five first records\",\nml_dataset.head(5)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initial data management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preprocessing aims at making the dataset compatible with modeling.\nAt the end of this step, we will have a matrix of float numbers, with no missing values.\nWe\u0027ll reuse the schema and the preprocessing steps defined in Models.\n\nLet\u0027s coerce categorical columns into unicode, numerical features into floats."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "categorical_features \u003d [u\u0027hour\u0027, u\u0027NAF_code\u0027, u\u0027Site\u0027, u\u0027month\u0027, u\u0027week_day\u0027]\nnumerical_features \u003d [u\u0027temperature\u0027]\ntext_features \u003d []\nfrom dataiku.doctor.utils import datetime_to_epoch\nfor feature in categorical_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].astype(\u0027unicode\u0027)\nfor feature in text_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].astype(\u0027unicode\u0027)\nfor feature in numerical_features:\n    if ml_dataset[feature].dtype \u003d\u003d np.dtype(\u0027\u003cM8[ns]\u0027):\n        ml_dataset[feature] \u003d datetime_to_epoch(ml_dataset[feature])\n    else:\n        ml_dataset[feature] \u003d ml_dataset[feature].astype(\u0027double\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preprocessing aims at making the dataset compatible with modeling.\nAt the end of this step, we will have a matrix of float numbers, with no missing values.\nWe\u0027ll reuse the schema and the preprocessing steps defined in Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "( Deduplication not required, skipping... )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We renamed the target variable to a column named target"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_dataset[\u0027__target__\u0027] \u003d ml_dataset[\u0027consomation\u0027]\ndel ml_dataset[\u0027consomation\u0027]\n\n\n# Remove rows for which the target is unknown.\nml_dataset \u003d ml_dataset[~ml_dataset[\u0027__target__\u0027].isnull()]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset needs to be split into 2 new sets, one that will be used for training the model\nand another that will be used to test its generalization capability.\nThis is a simple cross-validation strategy."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train, valid \u003d pdu.split_train_valid(ml_dataset, prop\u003d0.8)\nprint \u0027Train data has %i rows and %i columns\u0027 % (train.shape[0], train.shape[1])\nprint \u0027Validation data has %i rows and %i columns\u0027 % (valid.shape[0], valid.shape[1])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Features preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing to do at the features level is to handle the missing values.\nLet\u0027s reuse the settings defined in the model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drop_rows_when_missing \u003d []\nflag_when_missing \u003d []\nimpute_when_missing \u003d [{\u0027impute_with\u0027: u\u0027MEDIAN\u0027, \u0027feature\u0027: u\u0027temperature\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027hour\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027NAF_code\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027Site\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027month\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027week_day\u0027}]\n\n# Features for which we drop rows with missing values\"\nfor feature in drop_rows_when_missing:\n    train \u003d train[train[feature].notnull()]\n    valid \u003d valid[valid[feature].notnull()]\n    print \u0027Dropped missing records in %s\u0027 % feature\n\n# Features for which we replace the feature by a \u0027flag feature\u0027 indicating whether the value was present\"\nfor feature in flag_when_missing:\n    train[\u0027notMissing_\u0027 + feature] \u003d train[feature].map(lambda x: 0 if pd.isnull(x) else 1).astype(np.uint8)\n    del train[feature]\n    \n    valid[\u0027notMissing_\u0027 + feature] \u003d valid[feature].map(lambda x: 0 if pd.isnull(x) else 1).astype(np.uint8)\n    del valid[feature]\n    \n    print \u0027Flagged missing values in feature %s\u0027 % feature\n\n# Features for which we impute missing values\"\nfor feature in impute_when_missing:\n    if feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].mean()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEDIAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].median()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CREATE_CATEGORY\u0027:\n        v \u003d \u0027NULL_CATEGORY\u0027\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MODE\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].value_counts().index[0]\n    train[feature[\u0027feature\u0027]] \u003d train[feature[\u0027feature\u0027]].fillna(v)\n    valid[feature[\u0027feature\u0027]] \u003d valid[feature[\u0027feature\u0027]].fillna(v)\n    print \u0027Imputed missing values in feature %s with value %s\u0027 % (feature[\u0027feature\u0027], unicode(str(v), \u0027utf8\u0027))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now handle the categorical features (still using the settings defined in Models):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s dummify the following features."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LIMIT_DUMMIES \u003d 10\n\ncategorical_to_dummify \u003d [u\u0027hour\u0027, u\u0027NAF_code\u0027, u\u0027Site\u0027, u\u0027month\u0027, u\u0027week_day\u0027]\n\ndef select_dummy_values(df, features):\n    dummy_values \u003d {}\n    for feature in categorical_to_dummify:\n        values \u003d [\n            value\n            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)    \n        ]\n        dummy_values[feature] \u003d values\n    return dummy_values\n\nDUMMY_VALUES \u003d select_dummy_values(train, categorical_to_dummify)\n\ndef dummify_dataframe(df):\n    for (feature, dummy_values) in DUMMY_VALUES.items():\n        for dummy_value in dummy_values:\n            dummy_name \u003d \u0027%s_value_%s\u0027 % (feature, unicode(dummy_value, \u0027utf8\u0027))\n            df[dummy_name] \u003d (df[feature] \u003d\u003d dummy_value).astype(float)\n        del df[feature]\n        print \u0027Dummified feature %s\u0027 % feature\n\ndummify_dataframe(train)\n\ndummify_dataframe(valid)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rescaling features"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rescaling is not required"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dimension Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the last step before training the dimensionality reduction using a Principal Components Analysis."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# disabled in this model"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before actually creating our model, we need to split the datasets into their features and labels parts:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_X \u003d train.drop(\u0027__target__\u0027, axis\u003d1)\nvalid_X \u003d valid.drop(\u0027__target__\u0027, axis\u003d1)\n\ntrain_Y \u003d np.array(train[\u0027__target__\u0027])\nvalid_Y \u003d np.array(valid[\u0027__target__\u0027])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finally create our model !"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dataiku.doctor import RandomForestRegressorIML\nclf \u003d RandomForestRegressorIML(\n    n_jobs\u003d2,\n    random_state\u003d1337,\n    max_depth\u003d23,\n    min_samples_leaf\u003d31,\n    verbose\u003d2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... And train it"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%time clf.fit(train_X, train_Y)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build up our result dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%time _predictions \u003d clf.predict(valid_X)\npredictions \u003d pd.Series(data\u003d_predictions, index\u003dvalid_X.index, name\u003d\u0027predicted_value\u0027)\n\n# Build scored dataset\nresults_valid \u003d valid_X.join(predictions, how\u003d\u0027left\u0027)\nresults_valid \u003d results_valid.join(valid[\u0027__target__\u0027], how\u003d\u0027left\u0027)\nresults_valid \u003d results_valid.rename(columns\u003d {\u0027__target__\u0027: \u0027consomation\u0027})"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s have a look at feature importances"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "feature_importances_data \u003d []\nfeatures \u003d train_X.columns\nfor feature_name, feature_importance in zip(features, clf.feature_importances_):\n    feature_importances_data.append({\n        \u0027feature\u0027: feature_name,\n        \u0027importance\u0027: feature_importance\n    })\n\n# Plot the results\npd.DataFrame(feature_importances_data)\\\n    .set_index(\u0027feature\u0027)\\\n    .sort(\u0027importance\u0027)[-10::]\\\n    .plot(title\u003d\u0027Top 10 most important variables\u0027,\n          kind\u003d\u0027barh\u0027,\n          figsize\u003d(10, 6),\n          color\u003d\u0027#348ABD\u0027,\n          alpha\u003d0.6,\n          lw\u003d\u00271\u0027,\n          edgecolor\u003d\u0027#348ABD\u0027,\n          grid\u003dFalse,)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can measure the model\u0027s accuracy:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "c \u003d  results_valid[[\u0027predicted_value\u0027, \u0027consomation\u0027]].corr()\nprint \u0027Pearson correlation: %s\u0027 % c[\u0027predicted_value\u0027][1]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That\u0027s it. It\u0027s now up to you to tune your preprocessing, your algo, and your analysis !\n"
      ]
    }
  ]
}